{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-of-thoughts-llm in ./src (0.1.0)\n",
      "Requirement already satisfied: aiohttp>=3.8.4 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (3.10.10)\n",
      "Requirement already satisfied: aiosignal>=1.3.1 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.3.1)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (4.0.3)\n",
      "Requirement already satisfied: attrs>=23.1.0 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (24.2.0)\n",
      "Requirement already satisfied: backoff>=2.2.1 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2023.5.7 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer>=3.1.0 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (3.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.3.3 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.5.0)\n",
      "Requirement already satisfied: idna>=3.4 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (3.10)\n",
      "Requirement already satisfied: mpmath>=1.3.0 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.3.0)\n",
      "Requirement already satisfied: multidict>=6.0.4 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (6.1.0)\n",
      "Requirement already satisfied: openai>=0.27.7 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.53.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.12 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.13.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (4.66.6)\n",
      "Requirement already satisfied: urllib3>=2.0.2 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2.2.3)\n",
      "Requirement already satisfied: yarl>=1.9.2 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (1.17.1)\n",
      "Requirement already satisfied: numpy>=1.24.3 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.0.3 in ./tot_venv/lib/python3.9/site-packages (from tree-of-thoughts-llm) (2.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./tot_venv/lib/python3.9/site-packages (from aiohttp>=3.8.4->tree-of-thoughts-llm) (2.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./tot_venv/lib/python3.9/site-packages (from multidict>=6.0.4->tree-of-thoughts-llm) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (4.6.2.post1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (0.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./tot_venv/lib/python3.9/site-packages (from openai>=0.27.7->tree-of-thoughts-llm) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./tot_venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=0.27.7->tree-of-thoughts-llm) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./tot_venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai>=0.27.7->tree-of-thoughts-llm) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./tot_venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.27.7->tree-of-thoughts-llm) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tot_venv/lib/python3.9/site-packages (from pandas>=2.0.3->tree-of-thoughts-llm) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./tot_venv/lib/python3.9/site-packages (from pandas>=2.0.3->tree-of-thoughts-llm) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tot_venv/lib/python3.9/site-packages (from pandas>=2.0.3->tree-of-thoughts-llm) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./tot_venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=0.27.7->tree-of-thoughts-llm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./tot_venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=0.27.7->tree-of-thoughts-llm) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in ./tot_venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tree-of-thoughts-llm) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./tot_venv/lib/python3.9/site-packages (from yarl>=1.9.2->tree-of-thoughts-llm) (0.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tree-of-thoughts-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 19:14:47,934 - INFO - Backing off completions_with_backoff(...) for 0.8s (openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function gpt at 0x124bf0820>, model='gpt-4', temperature=0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 19:14:48,731 - INFO - Backing off completions_with_backoff(...) for 0.1s (openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742)\n",
      "2024-10-31 19:14:48,850 - INFO - Backing off completions_with_backoff(...) for 2.8s (openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742)\n",
      "2024-10-31 19:14:51,687 - INFO - Backing off completions_with_backoff(...) for 0.8s (openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742)\n",
      "2024-10-31 19:14:52,459 - INFO - Backing off completions_with_backoff(...) for 8.4s (openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/tot_venv/lib/python3.9/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/models.py:21\u001b[0m, in \u001b[0;36mcompletions_with_backoff\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@backoff\u001b[39m\u001b[38;5;241m.\u001b[39mon_exception(backoff\u001b[38;5;241m.\u001b[39mexpo, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletions_with_backoff\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/tot_venv/lib/python3.9/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m      5\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\n\u001b[1;32m      6\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     n_select_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m task \u001b[38;5;241m=\u001b[39m Game24Task()\n\u001b[0;32m---> 20\u001b[0m ys, infos \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(ys[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/methods/bfs.py:61\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(args, task, idx, to_print)\u001b[0m\n\u001b[1;32m     59\u001b[0m     new_ys \u001b[38;5;241m=\u001b[39m [get_samples(task, x, y, args\u001b[38;5;241m.\u001b[39mn_generate_sample, prompt_sample\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprompt_sample, stop\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mstops[step]) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmethod_generate \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpropose\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     new_ys \u001b[38;5;241m=\u001b[39m [get_proposals(task, x, y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys]\n\u001b[1;32m     62\u001b[0m new_ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain(\u001b[38;5;241m*\u001b[39mnew_ys))\n\u001b[1;32m     63\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_ys)))\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/methods/bfs.py:61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     new_ys \u001b[38;5;241m=\u001b[39m [get_samples(task, x, y, args\u001b[38;5;241m.\u001b[39mn_generate_sample, prompt_sample\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprompt_sample, stop\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mstops[step]) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmethod_generate \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpropose\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     new_ys \u001b[38;5;241m=\u001b[39m [\u001b[43mget_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys]\n\u001b[1;32m     62\u001b[0m new_ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain(\u001b[38;5;241m*\u001b[39mnew_ys))\n\u001b[1;32m     63\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_ys)))\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/methods/bfs.py:36\u001b[0m, in \u001b[0;36mget_proposals\u001b[0;34m(task, x, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_proposals\u001b[39m(task, x, y): \n\u001b[1;32m     35\u001b[0m     propose_prompt \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mpropose_prompt_wrap(x, y)\n\u001b[0;32m---> 36\u001b[0m     proposals \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpropose_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [y \u001b[38;5;241m+\u001b[39m _ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m proposals]\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/models.py:25\u001b[0m, in \u001b[0;36mgpt\u001b[0;34m(prompt, model, temperature, max_tokens, n, stop)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgpt\u001b[39m(prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     24\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/src/tot/models.py:33\u001b[0m, in \u001b[0;36mchatgpt\u001b[0;34m(messages, model, temperature, max_tokens, n, stop)\u001b[0m\n\u001b[1;32m     31\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     32\u001b[0m n \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m cnt\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions_with_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m outputs\u001b[38;5;241m.\u001b[39mextend([choice[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# log completion tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github_projects/tree-of-thought-llm/tot_venv/lib/python3.9/site-packages/backoff/_sync.py:127\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    124\u001b[0m     _call_handlers(on_backoff, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdetails, wait\u001b[38;5;241m=\u001b[39mseconds,\n\u001b[1;32m    125\u001b[0m                    exception\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     _call_handlers(on_success, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdetails)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tot.methods.bfs import solve\n",
    "from tot.tasks.game24 import Game24Task\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    backend='gpt-4', \n",
    "    temperature=0.7, \n",
    "    task='game24', \n",
    "    naive_run=False, \n",
    "    prompt_sample=None, \n",
    "    method_generate='propose', \n",
    "    method_evaluate='value', \n",
    "    method_select='greedy', \n",
    "    n_generate_sample=1, \n",
    "    n_evaluate_sample=3, \n",
    "    n_select_sample=5\n",
    "    )\n",
    "\n",
    "task = Game24Task()\n",
    "ys, infos = solve(args, task, 900)\n",
    "print(ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
