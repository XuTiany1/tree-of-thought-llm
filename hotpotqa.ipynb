{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./tot_venv/lib/python3.9/site-packages (1.53.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./tot_venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./tot_venv/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./tot_venv/lib/python3.9/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./tot_venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./tot_venv/lib/python3.9/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: tqdm>4 in ./tot_venv/lib/python3.9/site-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./tot_venv/lib/python3.9/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./tot_venv/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./tot_venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./tot_venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./tot_venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: certifi in ./tot_venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./tot_venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./tot_venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./tot_venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gym in ./tot_venv/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in ./tot_venv/lib/python3.9/site-packages (from gym) (8.5.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./tot_venv/lib/python3.9/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./tot_venv/lib/python3.9/site-packages (from gym) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./tot_venv/lib/python3.9/site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./tot_venv/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.20.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: requests in ./tot_venv/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./tot_venv/lib/python3.9/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./tot_venv/lib/python3.9/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./tot_venv/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./tot_venv/lib/python3.9/site-packages (from requests) (2024.8.30)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: beautifulsoup4 in ./tot_venv/lib/python3.9/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./tot_venv/lib/python3.9/site-packages (from beautifulsoup4) (2.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "\n",
    "!pip install gym\n",
    "\n",
    "!pip install requests\n",
    "\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Takes a prompt and an optional stop sequence (default: [\"\\n\"]), which signals where the model should stop generating text.\n",
    "def llm(prompt, stop=[\"\\n\"]):\n",
    "    \n",
    "    # Sends the prompt to OpenAI’s gpt-3.5-turbo-instruct model\n",
    "    response = openai.Completion.create(\n",
    "      model=\"gpt-3.5-turbo-instruct\",\n",
    "      prompt=prompt,\n",
    "      temperature=0,            # Low randomness, aiming for more predictable responses.\n",
    "      max_tokens=100,\n",
    "      top_p=1,                  # Selective so model model is selective\n",
    "      frequency_penalty=0.0,    # No penalty for anything\n",
    "      presence_penalty=0.0,\n",
    "      stop=stop\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikienv:\n",
    "A library for interacting with Wikipedia-based environments\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyixu/Documents/github_projects/tree-of-thought-llm/tot_venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import wikienv, wrappers\n",
    "\n",
    "# Initializes a WikiEnv instance\n",
    "env = wikienv.WikiEnv()\n",
    "env = wrappers.HotPotQAWrapper(env, split=\"dev\")        # Wraps env with HotPotQAWrappe\n",
    "env = wrappers.LoggingWrapper(env)                      # Wraps env again with LoggingWrapper, adding logging functionality to monitor and track the actions\n",
    "\n",
    "\n",
    "# Attempts to execute an action within the environment\n",
    "def step(env, action):\n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            return env.step(action)\n",
    "        except requests.exceptions.Timeout:\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Explanation:\n",
    "\n",
    "Run a question-answering (QA) task using a prompt-based approach that simulates multi-step reasoning with the help of an LLM (like GPT) and an interactive environment (env). (Here we are using Wikienv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts_naive.json explaiend:\n",
    "\n",
    "prompts_naive.json is a Prompting Guide, it contains structured example prompts with pre-defined questions and multi-step answers (using Thought → Action → Observation).\n",
    "These examples help prime the model to handle new questions in a similar structured way -> -> it helps set expectations for how the model should reason through questions.\n",
    "\n",
    "Why Use prompts_naive.json?\n",
    "Without examples to show the structure (like Thought → Action → Observation), the model might not naturally break down complex questions or simulate the reasoning steps effectively.\n",
    "prompts_naive.json essentially acts as a scaffolding to guide the model’s responses, making the evaluation more reliable and coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Loads the prompts_naive.json file, which includes sample QA interactions, into prompt_dict.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "folder = './src/reAct/prompts/'\n",
    "prompt_file = 'prompts_naive.json'\n",
    "with open(folder + prompt_file, 'r') as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# This setup combines instructions with webthink_examples, which provides a step-by-step Thought → Action → Observation format\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "webthink_examples = prompt_dict['webthink_simple6']\n",
    "instruction = \"\"\"Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: \n",
    "(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
    "(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n",
    "(3) Finish[answer], which returns the answer and finishes the task.\n",
    "Here are some examples.\n",
    "\"\"\"\n",
    "webthink_prompt = instruction + webthink_examples\n",
    "\n",
    "# prompts_naive.json is just the prompting framework that shows the model how to approach and answer each question in a structured format.\n",
    "\n",
    "\n",
    "\n",
    "#  Executes the QA task within an environment (env) using the structured Thought → Action → Observation process.\n",
    "\n",
    "# Simulates Multi-Step Reasoning: Each call to webthink aims to answer a question by breaking it down into sequential steps, each involving:\n",
    "#\tThought: Generates a “Thought” based on the current prompt.\n",
    "#\tAction: The model chooses an 'action' based on its thought, such as searching for information or finishing with an answer.\n",
    "#\tObservation: Receives an “Observation” from environemnt as feedback, which the model uses to inform the next step.\n",
    "\n",
    "def webthink(idx=None, prompt=webthink_prompt, to_print=True):\n",
    "\n",
    "    # First: Reset the environment for the specific question identified by idx\n",
    "    question = env.reset(idx=idx)\n",
    "    if to_print:\n",
    "        print(idx, question)\n",
    "    prompt += question + \"\\n\"\n",
    "    n_calls, n_badcalls = 0, 0\n",
    "\n",
    "\n",
    "    # For loop inside the webthink function is where the model iteratively generates thoughts and actions to answer a question in a structured way\n",
    "    for i in range(1, 8):\n",
    "        n_calls += 1\n",
    "\n",
    "        # First Step – Generating Initial Thought:\n",
    "        thought_action = llm(prompt + f\"Thought {i}:\", stop=[f\"\\nObservation {i}:\"])\n",
    "        try:\n",
    "\n",
    "            # Strip action and thought from each other\n",
    "            thought, action = thought_action.strip().split(f\"\\nAction {i}: \")\n",
    "        except:\n",
    "            print('ohh...', thought_action)\n",
    "            n_badcalls += 1\n",
    "            n_calls += 1\n",
    "            thought = thought_action.strip().split('\\n')[0]\n",
    "            action = llm(prompt + f\"Thought {i}: {thought}\\nAction {i}:\", stop=[f\"\\n\"]).strip()\n",
    "\n",
    "\n",
    "        # Performing Action and Generating Observation:\n",
    "            # action is executed in the environment (env) through the step function\n",
    "            # Observation is the feedback or result that the environment (env) returns after the model takes an action.\n",
    "            # Reward provides a numeric score for the action taken by the model\n",
    "        obs, r, done, info = step(env, action[0].lower() + action[1:])\n",
    "        obs = obs.replace('\\\\n', '')\n",
    "\n",
    "        # Adds the Thought, Action, and Observation sequence for each step to prompt, allowing the model to build on its prior responses.\n",
    "        step_str = f\"Thought {i}: {thought}\\nAction {i}: {action}\\nObservation {i}: {obs}\\n\"\n",
    "        prompt += step_str\n",
    "        if to_print:\n",
    "            print(step_str)\n",
    "\n",
    "        # Ending the Loop Early if model decides its good\n",
    "        if done:\n",
    "            break\n",
    "        # With each Observation added to the prompt, the model has more information to base its next Thought on.\n",
    "    \n",
    "    # If the loop completes its maximum iterations (7 in this case) without done being True, the function calls a finish[] action to explicitly end the task.\n",
    "    if not done:\n",
    "        obs, r, done, info = step(env, \"finish[]\")\n",
    "    if to_print:\n",
    "        print(info, '\\n')\n",
    "    info.update({'n_calls': n_calls, 'n_badcalls': n_badcalls, 'traj': prompt})\n",
    "\n",
    "    \n",
    "    return r, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3687 Question: What movie did actress Irene Jacob complete before the American action crime thriller film directed by Stuart Bird?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m old_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs[:\u001b[38;5;241m500\u001b[39m]:\n\u001b[0;32m---> 13\u001b[0m     r, info \u001b[38;5;241m=\u001b[39m \u001b[43mwebthink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     rs\u001b[38;5;241m.\u001b[39mappend(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mem\u001b[39m\u001b[38;5;124m'\u001b[39m])                   \u001b[38;5;66;03m# info['em'] representing the exact match score (1 if the answer is correct, 0 otherwise)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     infos\u001b[38;5;241m.\u001b[39mappend(info)\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mwebthink\u001b[0;34m(idx, prompt, to_print)\u001b[0m\n\u001b[1;32m     49\u001b[0m n_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# First Step – Generating Initial Thought:\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m thought_action \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThought \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mObservation \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Strip action and thought from each other\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     thought, action \u001b[38;5;241m=\u001b[39m thought_action\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mllm\u001b[0;34m(prompt, stop)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm\u001b[39m(prompt, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Sends the prompt to OpenAI’s gpt-3.5-turbo-instruct model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     11\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m       prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     13\u001b[0m       temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,            \u001b[38;5;66;03m# Low randomness, aiming for more predictable responses.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m       max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     15\u001b[0m       top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,                  \u001b[38;5;66;03m# Selective so model model is selective\u001b[39;00m\n\u001b[1;32m     16\u001b[0m       frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,    \u001b[38;5;66;03m# No penalty for anything\u001b[39;00m\n\u001b[1;32m     17\u001b[0m       presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m       stop\u001b[38;5;241m=\u001b[39mstop\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'completion'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "idxs = list(range(7405))\n",
    "random.Random(233).shuffle(idxs)\n",
    "\n",
    "# Stores the exact match scores (em) of each webthink run (indicating whether the answer was entirely correct)\n",
    "rs = []\n",
    "infos = []                  # Collects detailed info from each webthink call.\n",
    "old_time = time.time()\n",
    "\n",
    "\n",
    "for i in idxs[:500]:\n",
    "    r, info = webthink(i, to_print=True)\n",
    "    rs.append(info['em'])                   # info['em'] representing the exact match score (1 if the answer is correct, 0 otherwise)\n",
    "    infos.append(info)\n",
    "\n",
    "\n",
    "\t#\tsum(rs): Counts the total number of correct answers (exact matches).\n",
    "\t#\tlen(rs): Tracks how many questions have been processed.\n",
    "\t#\tsum(rs) / len(rs): Calculates the exact match rate (percentage of correct answers).\n",
    "\t#\t(time.time() - old_time) / len(rs): Computes the average time per question by dividing the elapsed time by the number of questions processed.\n",
    "    print(sum(rs), len(rs), sum(rs) / len(rs), (time.time() - old_time) / len(rs))\n",
    "    print('-----------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
